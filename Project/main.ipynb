{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from scipy.sparse import hstack\n",
    "import pickle as pkl\n",
    "from utils.tokenizer import tokenize_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNames(data):\n",
    "    names = []\n",
    "    if not data:\n",
    "        return names\n",
    "    parsedData = eval(data)\n",
    "    if not parsedData:\n",
    "        return names\n",
    "    for pieceOfInfo in parsedData:\n",
    "        name = pieceOfInfo['name']\n",
    "        names.append(name)\n",
    "    return np.array(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/links.csv', 'r', encoding='utf-8', newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader, None)\n",
    "    id_to_movieId = dict()\n",
    "    for line in reader:\n",
    "        try:\n",
    "            id_to_movieId[int(line[2])] = int(line[0])\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/movies_metadata.csv', encoding= 'utf-8') as csvFile:\n",
    "    reader = csv.DictReader(csvFile)\n",
    "    i = 0\n",
    "    for row in reader:\n",
    "        dataEmbeded[i, 0] = row['overview']\n",
    "        try:\n",
    "            dataEmbeded[i, 1] = id_to_movieId[int(row['id'])]\n",
    "        except:\n",
    "            pass\n",
    "        dataEmbeded[i, 2] = row['adult'] == 1\n",
    "        dataEmbeded[i, 3] = row['budget']\n",
    "        dataEmbeded[i, 4] = getNames(row['genres'])\n",
    "        dataEmbeded[i, 5] = row['popularity']\n",
    "        dataEmbeded[i, 6] = getNames(row['production_companies'])\n",
    "        dataEmbeded[i, 7] = row['production_countries'] == \"[{'iso_3166_1': 'US', 'name': 'United States of America'}]\"\n",
    "        dataEmbeded[i, 8] = row['revenue']\n",
    "        dataEmbeded[i, 9] = getNames(row['spoken_languages'])\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = MultiLabelBinarizer(sparse_output=True)\n",
    "genres = one_hot.fit_transform(dataEmbeded[:,4])\n",
    "production_companies = one_hot.fit_transform(dataEmbeded[:,6])\n",
    "spoken_languages = one_hot.fit_transform(dataEmbeded[:,9])\n",
    "BoW = tokenize_corpus(dataEmbeded[:,0], stop_words = False, BoW = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =  hstack([BoW, genres, spoken_languages])\n",
    "with open('./data/data.npy', 'wb') as pikeler:\n",
    "    data = {'ids':dataEmbeded[:, 1], 'data':data}\n",
    "    pkl.dump(data, pikeler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explication of base models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colaborative Deep Learning\n",
    "\n",
    "The first model on which we based ourselves is Hao Wang's model based on a Stacked Denoising Auto Encoder (SDAE), in charge of the item-based part. The principle this network is as follows:\n",
    "* We have a MLP neural network that is given a vector input and has to reproduce it as output.\n",
    "* A noise is applied to the input to make the network more robust\n",
    "* This network applies transformations to this vector until having a vector of small size compared to the input.\n",
    "* Then on a second part of the network, it reapplies transformations to this vector of small size until finding a vector of the same size as the entry. The loss is given by the difference between the input vector and the output vector in order to push the network to apply a reversible transformation within it.\n",
    "* In this way our network can be cut in half. A part that is an encoder that, given a large vector, encode a smaller, denser vector supposed to represent it. And a second part, able to decode this vector to find the original vector.\n",
    "\n",
    "This type of network is particularly interesting with bag of words approach because it gives at first a vector often very sparse with the size of the vocabulary, unusable without size reduction.\n",
    "\n",
    "<img src=\"./images/SDAE.png\" width=300px>\n",
    "\n",
    "On the other hand, for the collaborative part, embeddings are created for the users and items. Embeddings are widely used in other filed of domain (notably NLP), but are particularly adapted for this application. Indeed, embeddings are dense vectors representing an entity, the closer entities are, the closer their embeddings will be.\n",
    "\n",
    "After that, the item embedding and the dense vector created by the SDAE are concatenated making the full item embedding. \n",
    "Once this is done, the user and full item embedding are multiplied to form the ratings predictions.\n",
    "\n",
    "<img src=\"./images/MF.png\" width=600px>\n",
    "\n",
    "The full architected is as follow:\n",
    "\n",
    "<img src=\"./images/CDL.png\" width=400px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Collaborative Filter\n",
    "\n",
    "The second model is based on the first one, however Xiangnan He et al. that the matrix multiplication is suboptimal and doesn't have enough capacity to represent the non-linear relations between users, items and ratings. It is therefore proposed to replace the multiplication by a neural network.\n",
    "\n",
    "<img src=\"./images/NCF_1.png\" width=400px>\n",
    "\n",
    "The intuition behind this is that matrix multiplication is a special case of the MLP. Indeed, with the right weights (identity), a network can simply give the result of a matrix multiplication. Like so:\n",
    "\n",
    "<img src=\"./images/NCF_3.png\" width=200px>\n",
    "\n",
    "<img src=\"./images/NCF_2.png\" width=400px>\n",
    "\n",
    "However, empirical results showed that keeping the matrix multiplication still yield better results. The model they propose is then the following:\n",
    "<img src=\"./images/NCF_4.png\" width=400px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our model: Neural Hybrid Recommender\n",
    "\n",
    "We kept the main ideas proposed earlier but added a couple of improvements:\n",
    "* Addition of regularization layers (Batch-norm and Dropout)\n",
    "* Concatation of the SDAE to the Neural Collaborative Filter\n",
    "* Use of Adam optimizer\n",
    "\n",
    "The batch-norm improves the Convergence speed and Dropout prevents over-fitting. Adam optimizer adds Momentum en Nesterov Momentum and has proven to fasten the optimization.\n",
    "\n",
    "The model is then:\n",
    "\n",
    "<img src=\"./images/NHR.png\" width=400px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
