{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "\n",
    "encoding = 'utf-8'\n",
    "path_to_data = './data/'\n",
    "#Ouverture du fichier de sms\n",
    "path = path_to_data+\"dataEmbeded.pkl\"\n",
    "\n",
    "with open(path, 'rb') as pickler:\n",
    "    data = pkl.load(pickler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7df09d036cef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtokenize_corpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprocessed_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenize_corpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBoW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "from utils.tokenizer import tokenize_corpus\n",
    "\n",
    "processed_data = tokenize_corpus(data[:,0], stop_words = False, BoW = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device GeForce GTX 1050 Ti with Max-Q Design\n"
     ]
    }
   ],
   "source": [
    "import time, math, pickle, torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "print('Using cuda device '+torch.cuda.get_device_name(0) if use_cuda else 'Not using cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class batchify:\n",
    "    def __init__(self, data, bsz, training=False, split=0.8):\n",
    "        data = data[int(len(data)*split):] if training else data[:int(len(data)*split)]\n",
    "        self.training = training\n",
    "        self.batches = []\n",
    "        batch = []\n",
    "        for line in data:\n",
    "            if len(batch) != bsz:\n",
    "                batch.append(line)\n",
    "            else:\n",
    "                self.batches.append(batch)\n",
    "                batch = []\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.batches)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input_variable = Variable(torch.FloatTensor(self.batches[index]), require_grad=self.training)\n",
    "        target_variable = Variable(torch.FloatTensor(self.batches[index]), require_grad=self.training)\n",
    "        if use_cuda:\n",
    "            input_variable = input_variable.cuda()\n",
    "            target_variable = target_variable.cuda()\n",
    "        return (input_variable, target_variable)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (-%s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%3.fm %2.fs' % (m, s)\n",
    "  \n",
    "def save_model(name,encoder,decoder,iteration,local=False):\n",
    "    with open(\"{}/{}_iter.txt\".format(models_path, name), 'w') as f:\n",
    "        f.write(str(iteration))\n",
    "    torch.save(encoder.state_dict(),\"{}/{}_encoder.pt\".format(models_path, name))\n",
    "    torch.save(decoder.state_dict(),\"{}/{}_decoder.pt\".format(models_path, name))\n",
    "        \n",
    "def load_model(fname,model):\n",
    "    loaded_state_dict = torch.load(models_path+'/'+fname, map_location=None if use_cuda else {'cuda:0':'cpu'})\n",
    "    state_dict = model.state_dict()\n",
    "    state_dict.update(loaded_state_dict)\n",
    "    model.load_state_dict(loaded_state_dict)\n",
    "         \n",
    "def load_loss(name, loss_type): \n",
    "    losses = []\n",
    "    try:\n",
    "        with open(\"{}/{}_{}_losses.txt\".format(models_path, name, loss_type), 'r') as f:\n",
    "            for line in f:\n",
    "                losses = [float(value) for value in line.split(\";\")]\n",
    "                break\n",
    "    except:\n",
    "        print('No loss file')\n",
    "    return losses\n",
    "  \n",
    "def save_loss(name, loss_type, losses):\n",
    "    with open(\"{}/{}_{}_losses.txt\".format(models_path, name, loss_type), 'w') as f:\n",
    "        f.write(';'.join([str(value) for value in losses]))\n",
    "    \n",
    "def load_iter(name):\n",
    "    try:\n",
    "        with open(\"{}/{}_iter.txt\".format(models_path, name), 'r') as f:\n",
    "            for line in f:\n",
    "                start_iter = int(line)\n",
    "                break\n",
    "    except:\n",
    "        start_iter = 1\n",
    "        print('error during downloading of file')\n",
    "    return start_iter\n",
    "\n",
    "def log_func(x, a, b, c):\n",
    "    return a*x**2+b*x+c\n",
    "\n",
    "def showPlot(points, interpol=False):\n",
    "    fig, ax = plt.subplots(figsize=(20,15))\n",
    "    # this locator puts ticks at regular intervals\n",
    "    interval = (max(points)-min(points))/20\n",
    "    loc = ticker.MultipleLocator(base=interval)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "    if interpol:\n",
    "        x = np.arange(1e-6, len(points[15:]))\n",
    "        y = points[15:]\n",
    "        popt, pcov = scipy.optimize.curve_fit(log_func, x, y, p0=(1, 1, 1))\n",
    "        xx = np.linspace(1e-6, int(max(x)*1.5), 500)\n",
    "        yy = log_func(xx, *popt)\n",
    "        yy[0] = y[0]\n",
    "        plt.plot(xx,yy)\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNoise(nn.Module):\n",
    "    def __init__(self, sigma=0.1, is_relative_detach=True):\n",
    "        super().__init__()\n",
    "        self.sigma = sigma\n",
    "        self.is_relative_detach = is_relative_detach\n",
    "        self.noise = torch.tensor(0).cuda()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training and self.sigma != 0:\n",
    "            scale = self.sigma * x.detach() if self.is_relative_detach else self.sigma * x\n",
    "            sampled_noise = self.noise.repeat(*x.size()).normal_() * scale\n",
    "            x = x + sampled_noise\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class textEncoder(nn.Module):\n",
    "    def __init__(self, input_size, noise_sigma=0.3, \n",
    "                 layers=(512,256,128), dropout=0.9, batch_norm=False):\n",
    "        super().__init__()\n",
    "        self.noiseLayer = GaussianNoise(sigma=noise_sigma)\n",
    "        \n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for i, size in enumerate(layers):\n",
    "            if i == 0:\n",
    "                self.layers.append(nn.Linear(input_size, size))\n",
    "            else:\n",
    "                self.layers.append(nn.Linear(layers[i-1], size))\n",
    "            self.layers[-1].bias.data.fill_(0)\n",
    "            nn.init.xavier_normal_(self.layers[-1].weight.data)\n",
    "            self.layers.append(nn.ReLU())\n",
    "            self.layers.append(nn.Dropout(p=dropout))\n",
    "            if batch_norm:\n",
    "                self.layers.append(nn.BatchNorm1d(layers[i+1]))\n",
    "            \n",
    "        \n",
    "    def forward(self, input):\n",
    "        output = input\n",
    "        for layer in self.layers:\n",
    "            output = layer(output)\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class textDecoder(nn.Module):\n",
    "    def __init__(self, output_size, layers=(128,256,512), dropout=0.9, batch_norm=True):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for i, size in enumerate(layers[:-1]):\n",
    "            self.layers.append(nn.Linear(size, layers[i+1]))\n",
    "            self.layers[-1].bias.data.fill_(0)\n",
    "            nn.init.xavier_normal_(self.layers[-1].weight.data)\n",
    "            self.layers.append(nn.ReLU())\n",
    "            if batch_norm:\n",
    "                self.layers.append(nn.BatchNorm1d(layers[i+1]))\n",
    "            self.layers.append(nn.Dropout(p=dropout))\n",
    "        self.layers.append(nn.Linear(layers[-1], output_size))\n",
    "        self.layers[-1].bias.data.fill_(0)\n",
    "        nn.init.xavier_normal_(self.layers[-1].weight.data)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        output = input\n",
    "        for layer in self.layers:\n",
    "            output = layer(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class userEncoder(nn.Module):\n",
    "    def __init__(self, nb_users, layers=(128,), batch_norm=True, dropout= 0.5):\n",
    "          \n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for i, size in layers:\n",
    "            if i == 0:\n",
    "                self.layers.append(nn.Embedding(nb_users, size))\n",
    "            else:\n",
    "                self.layers.append(nn.Linear(layers[i-1], size))\n",
    "                self.layers[-1].bias.data.fill_(0)\n",
    "                nn.init.xavier_normal_(self.layers[-1].weight.data)\n",
    "                self.layers.append(nn.ReLU())\n",
    "                self.layers.append(nn.Dropout(p=dropout))\n",
    "                if batch_norm:\n",
    "                    self.layers.append(nn.BatchNorm1d(size))\n",
    "                \n",
    "    def forward(self, input):\n",
    "        output = input\n",
    "        for layer in self.layers:\n",
    "            output = layer(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class itemEncoder(nn.Module):\n",
    "    def __init__(self, nb_items, encoder_text_size=128, layers_encoder=(128,), layers_dense=(128,), batch_norm=True, dropout= 0.5):\n",
    "          \n",
    "        self.layers_encoder = torch.nn.ModuleList()\n",
    "        for i, size in layers_encoder:\n",
    "            if i == 0:\n",
    "                self.layers_encoder.append(nn.Embedding(nb_items, size))\n",
    "            else:\n",
    "                self.layers_encoder.append(nn.Linear(layers[i-1], size))\n",
    "                self.layers_encoder[-1].bias.data.fill_(0)\n",
    "                nn.init.xavier_normal_(self.layers[-1].weight.data)\n",
    "                self.layers_encoder.append(nn.ReLU())\n",
    "                self.layers_encoder.append(nn.Dropout(p=dropout))\n",
    "                if batch_norm:\n",
    "                    self.layers_encoder.append(nn.BatchNorm1d(size))\n",
    "        \n",
    "        self.layers_dense = torch.nn.ModuleList()\n",
    "        for i, size in layers_dense:\n",
    "            if i == 0:\n",
    "                self.layers_dense.append(nn.Linear(encoder_text_size+layers_encoder[-1], size))\n",
    "            else:\n",
    "                self.layers_dense.append(nn.Linear(layers_dense[i-1], size))\n",
    "            self.layers_dense[-1].bias.data.fill_(0)\n",
    "            nn.init.xavier_normal_(self.layers[-1].weight.data)\n",
    "            self.layers.append_dense(nn.ReLU())\n",
    "            self.layers.append_dense(nn.Dropout(p=dropout))\n",
    "            if batch_norm:\n",
    "                self.layers_dense.append(nn.BatchNorm1d(size))\n",
    "            \n",
    "                \n",
    "    def forward(self, input, encoder_output):\n",
    "        output = input\n",
    "        for layer in self.layers_encoder:\n",
    "            output = layer(output)\n",
    "            \n",
    "        output = torch.cat((output, encoder_output), dim=1)\n",
    "        for layer in self.layers_dense:\n",
    "            output = layer(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainAutoEncoder(X, encoder, decoder, itemEncoder=None, userEncoder=None, n_epochs=10, iter=1, start_epoch=1, local_save_every=1000, \n",
    "               print_every=10, plot_every=100, save_every=5000, batch_size=64, lr=0.01, lambda_u=0.001):\n",
    "    start = time.time()\n",
    "    total_iter = 0\n",
    "    best_loss = 99999\n",
    "    \n",
    "    model = CDL()\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=lr, weight_decay=lambda_u)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr, weight_decay=lambda_u)\n",
    "    \n",
    "    if itemEncoder != None and userEncoder != None:\n",
    "        itemEncoder_optimizer = optim.Adam(itemEncoder.parameters(), lr=lr, weight_decay=lambda_u)\n",
    "        userEncoder_optimizer = optim.Adam(userEncoder.parameters(), lr=lr, weight_decay=lambda_u)\n",
    "    \n",
    "    training_generator = batchify(X, batch_size, finetune = itemEncoder == None)\n",
    "    test_generator = batchify(X, batch_size, training=False, finetune = itemEncoder == None)\n",
    "    \n",
    "    no_improv = 0\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        try:\n",
    "            plot_losses = []\n",
    "            print_loss_total = []  # Reset every print_every\n",
    "            plot_loss_total = []  # Reset every plot_every\n",
    "            save_loss_total = []\n",
    "\n",
    "            encoder.train()\n",
    "            decoder.train()\n",
    "\n",
    "            for input_variable, target_variable in training_generator:     \n",
    "                if itemEncoder == None and userEncoder == None:\n",
    "                    loss = model.pretrain(input_variable, target_variable, \n",
    "                                          encoder, encoder_optimizer, decoder, decoder_optimizer, \n",
    "                                          train=True)\n",
    "                else:\n",
    "                    loss =  model.finetune(input_variable, target_variable, \n",
    "                                          encoder, encoder_optimizer, decoder, decoder_optimizer, \n",
    "                                          userEncoder, userEncoder_optimizer, ItemEncoder, ItemEncoder_optimizer, \n",
    "                                          train=True)\n",
    "                print_loss_total.append(loss)\n",
    "                plot_loss_total.append(loss)\n",
    "                save_loss_total.append(loss)\n",
    "\n",
    "                if iter % print_every == 0 or iter == len(training_generator):\n",
    "                    print_loss_avg = np.mean(print_loss_total)\n",
    "                    print_loss_total = []\n",
    "                    print('%s (%6.f %3.f%%) | Training loss: %.4e' % (timeSince(start, total_iter / len(training_generator) / n_epochs),\n",
    "                                                 iter, iter / len(training_generator) * 100, print_loss_avg))\n",
    "\n",
    "#                 if iter % plot_every == 0:\n",
    "#                     plot_loss_avg = np.mean(plot_loss_total)\n",
    "#                     train_losses = load_loss(hp.name, 'train')\n",
    "#                     train_losses.append(plot_loss_avg)\n",
    "#                     save_loss(hp.name, 'train', train_losses)\n",
    "#                     plot_loss_total = []\n",
    "                iter += 1\n",
    "                total_iter += 1\n",
    "\n",
    "                if iter >= len(training_generator):\n",
    "                    break\n",
    "            encoder.eval()\n",
    "            decoder.eval()\n",
    "\n",
    "            iter=1\n",
    "            print_loss_total = []\n",
    "            t0 = time.time()\n",
    "            for input_variable, target_variable in test_generator:\n",
    "                if itemEncoder == None and userEncoder == None:\n",
    "                    loss = model.pretrain(input_variable, target_variable, \n",
    "                                          encoder, encoder_optimizer, decoder, decoder_optimizer, \n",
    "                                          train=False)\n",
    "                else:\n",
    "                    loss =  model.finetune(input_variable, target_variable, \n",
    "                                          encoder, encoder_optimizer, decoder, decoder_optimizer, \n",
    "                                          userEncoder, userEncoder_optimizer, ItemEncoder, ItemEncoder_optimizer, \n",
    "                                          train=False)\n",
    "                \n",
    "                print_loss_total.append(loss)\n",
    "                if iter % len(test_generator) == 0:\n",
    "                    tf = time.time()\n",
    "                    print_loss_avg = np.mean(print_loss_total)\n",
    "                    print_loss_total = []\n",
    "                    print('Validation loss: %.4e | Time/sample: %dms' % (print_loss_avg, int((tf-t0)/len(test_generator)/batch_size*1000)))\n",
    "                    \n",
    "#                     valid_losses = load_loss(hp.name, 'valid')\n",
    "#                     if len(valid_losses) < 2:\n",
    "                    if epoch < 1:\n",
    "#                         save_model(hp.name, encoder, decoder, iter)\n",
    "                        pass\n",
    "                    else:\n",
    "                        if min(valid_losses[-2:]) < print_loss_avg :\n",
    "                            no_improv += 1\n",
    "                        else:\n",
    "                            no_improv = 0\n",
    "#                             save_model(hp.name, encoder, decoder, iter)\n",
    "#                             print('Model Saved')\n",
    "                    if no_improv > 1:\n",
    "                        lr = encoder_optimizer.param_groups[0]['lr']\n",
    "                        encoder_optimizer.param_groups[0]['lr'] = lr / 2\n",
    "                        decoder_optimizer.param_groups[0]['lr'] = lr / 2\n",
    "#                         print('No Improvement for 2 epoch, dividing the learning rate by 2')\n",
    "#                     valid_losses.append(print_loss_avg)\n",
    "#                     save_loss(hp.name, 'valid', valid_losses)\n",
    "                iter += 1\n",
    "                total_iter += 1\n",
    "            iter=1\n",
    "            \n",
    "            if epoch+start_epoch >= n_epochs:\n",
    "                break\n",
    "        except KeyboardInterrupt:\n",
    "            print('User stopped training')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CDL:\n",
    "    def __init__(self):\n",
    "        return \n",
    "    \n",
    "    def pretrain(self, input_variable, target_variable, encoder, encoder_optimizer, decoder, decoder_optimizer, train=False):\n",
    "\n",
    "        criterion = nn.MSELoss(reduction='elementwise_mean')\n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        loss = 0\n",
    "\n",
    "        encoder_outputs = encoder(input_variable)\n",
    "        decoder_outputs = decoder(encoder_outputs)\n",
    "        raw_loss = criterion(decoder_outputs, target_variable)\n",
    "\n",
    "        loss = raw_loss\n",
    "\n",
    "        if train:\n",
    "            loss.backward()\n",
    "\n",
    "    #         torch.nn.utils.clip_grad_norm(encoder.parameters(), grad_clip)\n",
    "            encoder_optimizer.step()\n",
    "\n",
    "    #         torch.nn.utils.clip_grad_norm(decoder.parameters(), grad_clip)\n",
    "            decoder_optimizer.step()\n",
    "\n",
    "        return raw_loss.item()\n",
    "    \n",
    "    def finetune(self, input_variable, target_variable, encoder, encoder_optimizer, decoder, decoder_optimizer, userEncoder, userEncoder_optimizer, ItemEncoder, ItemEncoder_optimizer, train=False)\n",
    "        criterion = nn.MSELoss(reduction='elementwise_mean')\n",
    "        text = input_variable[0]\n",
    "        user_index = input_variable[1]\n",
    "        item_index = input_variable[2]\n",
    "        \n",
    "        ratings = target_variable\n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        userEncoder_optimizer.zero_grad()\n",
    "        itemEncoder_optimizer.zero_grad()\n",
    "\n",
    "        loss = 0\n",
    "\n",
    "        encoder_outputs = encoder(input_variable)\n",
    "        user_embedding = userEncoder(user_index)\n",
    "        item_embedding = itemEncoder(item_index, encoder_outputs)\n",
    "        \n",
    "        predictions = torch.dot(user_embedding, item_embedding.t())\n",
    "        \n",
    "        raw_loss = criterion(predictions, ratings)\n",
    "\n",
    "        loss = raw_loss\n",
    "\n",
    "        if train:\n",
    "            loss.backward()\n",
    "\n",
    "    #         torch.nn.utils.clip_grad_norm(encoder.parameters(), grad_clip)\n",
    "            encoder_optimizer.step()\n",
    "\n",
    "    #         torch.nn.utils.clip_grad_norm(decoder.parameters(), grad_clip)\n",
    "            decoder_optimizer.step()\n",
    "        \n",
    "    #         torch.nn.utils.clip_grad_norm(encoder.parameters(), grad_clip)\n",
    "            itemEncoder_optimizer.step()\n",
    "\n",
    "    #         torch.nn.utils.clip_grad_norm(decoder.parameters(), grad_clip)\n",
    "            userEncoder_optimizer.step()\n",
    "\n",
    "        return raw_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder1 = textEncoder(input_size = processed_data.shape[1])\n",
    "decoder1 = textDecoder(output_size = processed_data.shape[1])\n",
    "\n",
    "if use_cuda:\n",
    "    encoder1 = encoder1.cuda()\n",
    "    decoder1 = decoder1.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0m 14s (-148m 13s) (    10   2%) | Training loss: 2.9892e-02\n",
      "  0m 23s (-110m  3s) (    20   4%) | Training loss: 9.5206e-03\n",
      "  0m 31s (- 98m 57s) (    30   5%) | Training loss: 3.7695e-03\n",
      "  0m 39s (- 91m 59s) (    40   7%) | Training loss: 1.6978e-03\n",
      "  0m 47s (- 87m 48s) (    50   9%) | Training loss: 1.0854e-03\n",
      "  0m 55s (- 85m 40s) (    60  11%) | Training loss: 7.9409e-04\n",
      "  1m  3s (- 83m 33s) (    70  13%) | Training loss: 7.5453e-04\n",
      "  1m 10s (- 81m 57s) (    80  14%) | Training loss: 7.8771e-04\n",
      "  1m 19s (- 81m 10s) (    90  16%) | Training loss: 7.4474e-04\n",
      "  1m 27s (- 80m 18s) (   100  18%) | Training loss: 7.4785e-04\n",
      "  1m 35s (- 79m 32s) (   110  20%) | Training loss: 7.1309e-04\n",
      "  1m 43s (- 78m 51s) (   120  21%) | Training loss: 7.2489e-04\n",
      "  1m 51s (- 78m 14s) (   130  23%) | Training loss: 7.9322e-04\n",
      "  1m 59s (- 77m 46s) (   140  25%) | Training loss: 7.7654e-04\n",
      "  2m  7s (- 77m 22s) (   150  27%) | Training loss: 8.2557e-04\n",
      "  2m 15s (- 76m 57s) (   160  29%) | Training loss: 7.7541e-04\n",
      "  2m 23s (- 76m 26s) (   170  30%) | Training loss: 7.1486e-04\n",
      "  2m 31s (- 75m 58s) (   180  32%) | Training loss: 7.2375e-04\n",
      "  2m 39s (- 75m 39s) (   190  34%) | Training loss: 6.8736e-04\n",
      "  2m 47s (- 75m 19s) (   200  36%) | Training loss: 7.2162e-04\n",
      "  2m 55s (- 75m  1s) (   210  38%) | Training loss: 7.5099e-04\n",
      "  3m  3s (- 74m 48s) (   220  39%) | Training loss: 7.2066e-04\n",
      "  3m 11s (- 74m 31s) (   230  41%) | Training loss: 7.7259e-04\n",
      "  3m 19s (- 74m 15s) (   240  43%) | Training loss: 7.8974e-04\n",
      "  3m 27s (- 74m  1s) (   250  45%) | Training loss: 7.8219e-04\n",
      "  3m 35s (- 73m 49s) (   260  47%) | Training loss: 8.2195e-04\n",
      "  3m 44s (- 73m 42s) (   270  48%) | Training loss: 7.5343e-04\n",
      "  3m 52s (- 73m 44s) (   280  50%) | Training loss: 8.0110e-04\n",
      "  4m  0s (- 73m 30s) (   290  52%) | Training loss: 8.1437e-04\n",
      "  4m  9s (- 73m 18s) (   300  54%) | Training loss: 7.8162e-04\n",
      "  4m 17s (- 73m  5s) (   310  55%) | Training loss: 7.8542e-04\n",
      "  4m 25s (- 72m 53s) (   320  57%) | Training loss: 7.4356e-04\n",
      "  4m 33s (- 72m 39s) (   330  59%) | Training loss: 7.8224e-04\n",
      "  4m 41s (- 72m 28s) (   340  61%) | Training loss: 7.3767e-04\n",
      "  4m 49s (- 72m 24s) (   350  63%) | Training loss: 8.0922e-04\n",
      "  4m 58s (- 72m 16s) (   360  64%) | Training loss: 7.6038e-04\n",
      "  5m  6s (- 72m  6s) (   370  66%) | Training loss: 7.2440e-04\n",
      "  5m 14s (- 71m 59s) (   380  68%) | Training loss: 7.1749e-04\n",
      "  5m 22s (- 71m 50s) (   390  70%) | Training loss: 7.7754e-04\n",
      "  5m 31s (- 71m 43s) (   400  72%) | Training loss: 6.8585e-04\n",
      "  5m 39s (- 71m 32s) (   410  73%) | Training loss: 6.8670e-04\n",
      "  5m 47s (- 71m 23s) (   420  75%) | Training loss: 7.5051e-04\n",
      "  5m 56s (- 71m 18s) (   430  77%) | Training loss: 7.5610e-04\n",
      "  6m  5s (- 71m 25s) (   440  79%) | Training loss: 8.0635e-04\n",
      "  6m 14s (- 71m 19s) (   450  81%) | Training loss: 7.3281e-04\n",
      "  6m 22s (- 71m 10s) (   460  82%) | Training loss: 7.6208e-04\n",
      "  6m 30s (- 71m  2s) (   470  84%) | Training loss: 8.0379e-04\n",
      "  6m 39s (- 70m 53s) (   480  86%) | Training loss: 8.5204e-04\n",
      "  6m 47s (- 70m 43s) (   490  88%) | Training loss: 7.9090e-04\n",
      "  6m 55s (- 70m 34s) (   500  89%) | Training loss: 8.2222e-04\n",
      "  7m  3s (- 70m 26s) (   510  91%) | Training loss: 8.0456e-04\n",
      "  7m 11s (- 70m 16s) (   520  93%) | Training loss: 8.7482e-04\n",
      "  7m 19s (- 70m  4s) (   530  95%) | Training loss: 9.2666e-04\n",
      "  7m 27s (- 69m 53s) (   540  97%) | Training loss: 7.4386e-04\n",
      "  7m 35s (- 69m 41s) (   550  98%) | Training loss: 9.0732e-04\n",
      "Validation loss: 7.6778e-04 | Time/sample: 9ms\n",
      " 13m 31s (- 53m 35s) (    10   2%) | Training loss: 7.3830e-04\n",
      " 13m 38s (- 53m 28s) (    20   4%) | Training loss: 7.6585e-04\n",
      " 13m 45s (- 53m 20s) (    30   5%) | Training loss: 7.3249e-04\n",
      " 13m 53s (- 53m 14s) (    40   7%) | Training loss: 6.9239e-04\n",
      " 13m 60s (- 53m  7s) (    50   9%) | Training loss: 7.6312e-04\n",
      " 14m  7s (- 52m 59s) (    60  11%) | Training loss: 6.9971e-04\n",
      " 14m 14s (- 52m 52s) (    70  13%) | Training loss: 7.2571e-04\n",
      " 14m 21s (- 52m 44s) (    80  14%) | Training loss: 7.8097e-04\n",
      " 14m 29s (- 52m 37s) (    90  16%) | Training loss: 7.4292e-04\n",
      " 14m 36s (- 52m 31s) (   100  18%) | Training loss: 7.4722e-04\n",
      " 14m 43s (- 52m 23s) (   110  20%) | Training loss: 7.1281e-04\n",
      " 14m 50s (- 52m 17s) (   120  21%) | Training loss: 7.2475e-04\n",
      " 14m 58s (- 52m  9s) (   130  23%) | Training loss: 7.9324e-04\n",
      " 15m  5s (- 52m  2s) (   140  25%) | Training loss: 7.7665e-04\n",
      " 15m 12s (- 51m 55s) (   150  27%) | Training loss: 8.2550e-04\n",
      " 15m 19s (- 51m 48s) (   160  29%) | Training loss: 7.7551e-04\n",
      " 15m 27s (- 51m 41s) (   170  30%) | Training loss: 7.1490e-04\n",
      " 15m 34s (- 51m 33s) (   180  32%) | Training loss: 7.2378e-04\n",
      " 15m 41s (- 51m 26s) (   190  34%) | Training loss: 6.8742e-04\n",
      " 15m 48s (- 51m 18s) (   200  36%) | Training loss: 7.2158e-04\n",
      " 15m 55s (- 51m 10s) (   210  38%) | Training loss: 7.5111e-04\n",
      " 16m  2s (- 51m  3s) (   220  39%) | Training loss: 7.2066e-04\n",
      " 16m  9s (- 50m 56s) (   230  41%) | Training loss: 7.7260e-04\n",
      " 16m 16s (- 50m 49s) (   240  43%) | Training loss: 7.8984e-04\n",
      " 16m 23s (- 50m 41s) (   250  45%) | Training loss: 7.8213e-04\n",
      " 16m 30s (- 50m 33s) (   260  47%) | Training loss: 8.2210e-04\n",
      " 16m 38s (- 50m 26s) (   270  48%) | Training loss: 7.5343e-04\n",
      " 16m 45s (- 50m 18s) (   280  50%) | Training loss: 8.0114e-04\n",
      " 16m 52s (- 50m 11s) (   290  52%) | Training loss: 8.1440e-04\n",
      " 16m 59s (- 50m  4s) (   300  54%) | Training loss: 7.8170e-04\n",
      " 17m  6s (- 49m 57s) (   310  55%) | Training loss: 7.8544e-04\n",
      " 17m 14s (- 49m 50s) (   320  57%) | Training loss: 7.4361e-04\n",
      " 17m 21s (- 49m 42s) (   330  59%) | Training loss: 7.8228e-04\n",
      " 17m 28s (- 49m 35s) (   340  61%) | Training loss: 7.3774e-04\n",
      " 17m 35s (- 49m 27s) (   350  63%) | Training loss: 8.0920e-04\n",
      " 17m 42s (- 49m 19s) (   360  64%) | Training loss: 7.6047e-04\n",
      " 17m 49s (- 49m 11s) (   370  66%) | Training loss: 7.2441e-04\n",
      " 17m 56s (- 49m  4s) (   380  68%) | Training loss: 7.1755e-04\n",
      " 18m  3s (- 48m 57s) (   390  70%) | Training loss: 7.7757e-04\n",
      " 18m 10s (- 48m 50s) (   400  72%) | Training loss: 6.8592e-04\n",
      " 18m 18s (- 48m 44s) (   410  73%) | Training loss: 6.8672e-04\n",
      " 18m 25s (- 48m 37s) (   420  75%) | Training loss: 7.5055e-04\n",
      " 18m 32s (- 48m 29s) (   430  77%) | Training loss: 7.5616e-04\n",
      " 18m 39s (- 48m 22s) (   440  79%) | Training loss: 8.0637e-04\n",
      " 18m 46s (- 48m 14s) (   450  81%) | Training loss: 7.3288e-04\n",
      " 18m 53s (- 48m  7s) (   460  82%) | Training loss: 7.6212e-04\n",
      " 19m  1s (- 47m 59s) (   470  84%) | Training loss: 8.0384e-04\n",
      " 19m  8s (- 47m 52s) (   480  86%) | Training loss: 8.5207e-04\n",
      " 19m 15s (- 47m 45s) (   490  88%) | Training loss: 7.9096e-04\n",
      " 19m 22s (- 47m 38s) (   500  89%) | Training loss: 8.2223e-04\n",
      " 19m 29s (- 47m 30s) (   510  91%) | Training loss: 8.0464e-04\n",
      " 19m 36s (- 47m 23s) (   520  93%) | Training loss: 8.7487e-04\n",
      " 19m 43s (- 47m 16s) (   530  95%) | Training loss: 9.2669e-04\n",
      " 19m 51s (- 47m  8s) (   540  97%) | Training loss: 7.4392e-04\n",
      " 19m 58s (- 47m  1s) (   550  98%) | Training loss: 9.0734e-04\n",
      "Validation loss: 7.6788e-04 | Time/sample: 9ms\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'valid_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-7696d255df06>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mstart_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrainAutoEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessed_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-b9c171f6033c>\u001b[0m in \u001b[0;36mtrainAutoEncoder\u001b[1;34m(X, encoder, decoder, n_epochs, iter, start_epoch, local_save_every, print_every, plot_every, save_every, batch_size, lr, lambda_u)\u001b[0m\n\u001b[0;32m     82\u001b[0m                         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m                         \u001b[1;32mif\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_losses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mprint_loss_avg\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m                             \u001b[0mno_improv\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m                         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'valid_losses' is not defined"
     ]
    }
   ],
   "source": [
    "start_iter = 1\n",
    "trainAutoEncoder(processed_data, encoder1, decoder1, 10, iter=start_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
