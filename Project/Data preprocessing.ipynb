{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# 17th of January 2017\n",
    "# Alexandre Dos Santos and Amélie Simon\n",
    "# To clean Tel Jeune SMS -> Return one file with all messages of a user appended and the subject conversation code\n",
    "# same codes used for emails\n",
    "#########################################\n",
    "\n",
    "import csv\n",
    "import pickle\n",
    "import html\n",
    "from time import time\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "encoding = 'utf-8'\n",
    "path_to_data = './data'\n",
    "#Ouverture du fichier de sms\n",
    "path = path_to_data+\"movies-metadata.csv\"\n",
    "sms = []\n",
    "\n",
    "with open(path, newline='',encoding = encoding ) as csvfile:\n",
    "        sms_reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "        for row in sms_reader:\n",
    "            sms.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "825994it [00:21, 38398.10it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "#Ouverture du fichier de conversations et contruction du dataset\n",
    "path = path_to_data+\"app_conversations.csv\"\n",
    "conversations = []\n",
    "subjects = []\n",
    "broke_conv_ids = []\n",
    "conv_ids = set() #liste des id de conv. déjà ouvertes\n",
    "length = 0\n",
    "\n",
    "with open(path, newline='' ,encoding = encoding ) as csvfile:\n",
    "        conv_reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "        for row in conv_reader:\n",
    "            length+=1\n",
    "\n",
    "            \n",
    "to_standard_subject = dict()\n",
    "to_standard_subject[''] = []\n",
    "with open(\"./data/subject_sms.csv\", newline='',encoding = encoding ) as csvfile:\n",
    "        csv_reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "        for row in csv_reader:\n",
    "            to_standard_subject[row[0]] = row[1:]\n",
    "\n",
    "with open(path, newline='' ,encoding = encoding ) as csvfile:\n",
    "        conv_reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "        for row in conv_reader:\n",
    "            break\n",
    "        for row in tqdm(conv_reader):            \n",
    "            conv_id = int(row[0]) - 1    #Id de conversation\n",
    "            subject = row[3]         #Sujet de la conversation\n",
    "            subject_id = row[4]      #Id du sujet de la conversation\n",
    "            sms_index = int(row[5])  #Index du sms concerné\n",
    "            sender = sms[sms_index][1]\n",
    "            sender_id = sms[sms_index][2]\n",
    "            \n",
    "            # Si la conversation existe déjà\n",
    "            if conv_id in conv_ids:\n",
    "                conversation = conversations[conv_id]\n",
    "                if sender == 'mo': #Si l'envoyeur du message est le jeune\n",
    "                    if len(conversation[-1][1]) == 0: # Si le message est une continuation du message précédent\n",
    "                        conversation[-1][0].append(sms_index)\n",
    "                    else:\n",
    "                        conversation.append([[sms_index], []])\n",
    "                else: #Si l'envoyeur du message est l'intervenant \n",
    "                    if len(conversation[-1][1]) == 0: # Si le message est une réponse directe\n",
    "                        conversation[-1][1] = [sms_index]\n",
    "                    else:\n",
    "                        conversation[-1][1].append(sms_index)\n",
    "            else:\n",
    "                if sender == 'mo':\n",
    "                    conv_ids.add(conv_id) #Rajouter l'id de conversation à la liste des id de conv. déjà ouvertes\n",
    "                    conversations.append([[[sms_index], []]])\n",
    "                    subjects.append(to_standard_subject[subject])\n",
    "                else: #Si l'envoyeur du premier message de la conversation est l'intervenant \n",
    "                    broke_conv_ids.append(conv_id)\n",
    "                    for j in range(len(sms[:sms_index])):\n",
    "                        if sender_id == sms[sms_index-j][3]:\n",
    "                            first_message_id = [sms_index-j]\n",
    "                            conv_ids.add(conv_id) #Rajouter l'id de conversation à la liste des id de conv. déjà ouvertes\n",
    "                            conversations.append([[first_message_id, [sms_index]]])\n",
    "                            subjects.append(to_standard_subject[subject])\n",
    "                            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Q&A data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def replace_ponctuation(raw_text):\n",
    "    raw_text = raw_text.replace(\"...\", \"&&&\")\n",
    "    raw_text = raw_text.replace(\".\", \" . \").replace(\",\", \" , \").replace(\";\", \" ; \").replace(\":\", \" : \").replace(\"!\", \" ! \").replace(\"?\", \" ? \").replace(\"&&&\", \" &&& \").replace(\"(\", \" ( \").replace(\")\", \" ) \").replace('\"', ' \" ')\n",
    "                        \n",
    "    return re.sub(' +', \" \", raw_text).replace(\"&&&\", \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 31017/31017 [00:17<00:00, 1776.56it/s]\n"
     ]
    }
   ],
   "source": [
    "Q_A = []\n",
    "line_no = 0\n",
    "for j, conversation in enumerate(tqdm(conversations)):\n",
    "    context = []\n",
    "    for line in conversation:\n",
    "        splited_question = [html.unescape(sms[i][4]) for i in line[0]]\n",
    "        if len(line[0]) > 1 and sms[line[0][1]][4][0] != ' ':\n",
    "            joint = ' '\n",
    "        else:\n",
    "            joint = ''\n",
    "        question = replace_ponctuation(joint.join(splited_question))\n",
    "        answer = replace_ponctuation(' '.join([html.unescape(sms[i][4]) for i in line[1]]))\n",
    "        if len(answer) > 0:\n",
    "            Q_A.append([context[:], question, answer, subjects[j]])\n",
    "            context.append(line_no)\n",
    "            line_no += 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from utils import custom_tokenizer\n",
    "\n",
    "tokenize = custom_tokenizer.init(use_nltk_tok=True, use_stemmer=False)\n",
    "\n",
    "questions = [line[1] for line in Q_A]\n",
    "answers = [line[2] for line in Q_A]\n",
    "tokenized_questions = []\n",
    "tokenized_answers = []\n",
    "\n",
    "p = Pool()\n",
    "tokenized_questions = p.map(tokenize, questions)\n",
    "tokenized_answers = p.map(tokenize, answers)\n",
    "p.close()\n",
    "\n",
    "for i, line in enumerate(Q_A):\n",
    "    line[1] = tokenized_questions[i]\n",
    "    line[2] = tokenized_answers[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "tokenize = spacy.load('fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Icône\n",
      "[('Icône', 'LOC')]\n",
      "Comment vous savez mon nom ? ? ? \n",
      "[]\n",
      "Ah bon , on a plus de secret maintenant\n",
      "[]\n",
      "Allo Ok merci d'être là : ) \n",
      "[]\n",
      "Je me sens toute seule\n",
      "[]\n",
      "Varennes , Montréal , Canada\n",
      "[('Varennes', 'LOC'), ('Montréal', 'LOC'), ('Canada', 'LOC')]\n",
      "Car j'avais une amie qui se nomme Marie-Maude puis nous ne sommes plus amies car elle se fâchait régulièrement envers moi . Elle dit qu'elle a une tête de cochon . Puis quand je lui met en pleine face que c'est ça la réalité elle ne comprend pas . Puis elle fait des crises de colère . Mais moi je dis seulement que je suis gentille avec elle . \n",
      "[('Marie', 'PER'), ('Maude', 'PER')]\n",
      "J'aimerais que vous m'aidiez en me disant qu'elle serait la bonne chose à faire ? \n",
      "[]\n",
      "Passer à autre chose car ce n'est pas la première fois que elle me fait le coup . \n",
      "[]\n",
      "Oui mais on habite l'une près de l'autre\n",
      "[]\n",
      "Oui mais une veut m'arracher la tête Par ce qu'elle est en furie je l'as connais depuis qu'elle a 10 ans aussi ses parents sont séparés . elle est de même avec tout le monde\n",
      "[]\n",
      "J'ai envie qu'on se parle mais si elle ne veut pas je ne peux rien faire\n",
      "[]\n",
      "Je ne me protège même pas\n",
      "[]\n",
      "Elle frappe tout le monde\n",
      "[]\n",
      "Je le sais mais moi elle ne m'a jamais touché . Sauf que beaucoup de monde me disent qu'elle est colérique . \n",
      "[]\n",
      "Oui mais si elle me frappe je dois faire quoi ? \n",
      "[]\n",
      "Ok c'est bon merci : ) je vais vous reparler si j'en ai besoin . Oui mais j'ai 18 ans\n",
      "[]\n",
      "16 ans moi 18 ans Elle a 16 ans\n",
      "[]\n",
      "Oui mais si elle dit qu'elle a rien fait et qu'il n'y pas de témoin je ne peux pas porter plainte . \n",
      "[]\n",
      "D'accord au moins l'an prochain ça va aider on ne se croiserait pas elle va changer d'école . \n",
      "[]\n",
      "Ok merci je peux vous reparler dans la journée si je ne vais pas bien ? \n",
      "[]\n",
      "Bonjour , j'aurais besoin de parler ! \n",
      "[('Bonjour', 'MISC')]\n",
      "J'aurais besoin de parler ... : /\n",
      "[]\n",
      "16 ans fille mtl ! C correct\n",
      "[('C', 'MISC')]\n",
      "Je croi que jai un problème mental ! ! \n",
      "[]\n",
      "Je veut un bébé\n",
      "[]\n",
      "Pcq je c a quel point c'est bcp de problème a mon age mes jen veut un quand même\n",
      "[]\n",
      "Je c'est pas jennveu unnn ! ! ! ! \n",
      "[]\n",
      "C beau ! C'est agréable ! \n",
      "[('C', 'MISC')]\n",
      "Je c'est mes je veu tlm avoir un enfant ! ! \n",
      "[]\n",
      "allo ? merci\n",
      "[]\n",
      "ya t'il des frais\n",
      "[]\n",
      "d'acc bon je m'explique . . \n",
      "[]\n",
      "J'ai 17 ans je suis une fille de st jean sur richelieu\n",
      "[]\n",
      "j'ai sortit 1 an et demi avec un garçon et sa c'est terminé il y a 1 an . Recemment j'ai rencontré un garçon dont je suis vraiment attaché , ont à couché emsenble et tout . Lui c'est le gars qui n'a jamais eu de blonde mais meme si il dit le contraire il c'est attacher a moi . Sauf qu'il prefere me fuire ... il ne repond plus a mes textos et mes appels et jarrete pas de pleurer ... je sais plus quoi faire\n",
      "[]\n",
      "oui . . \n",
      "[]\n",
      "il ne sait pas qu'il dit\n",
      "[]\n",
      "si il veut qu'ont soit un couple\n",
      "[]\n",
      "la deuxieme fois quon cest vue ... et je lui en est reparler il m'a dit je suis pas comme ça , mais avec moi il agit comme si il etait attacher\n",
      "[]\n",
      " ? \n",
      "[]\n",
      "oh lala ... \n",
      "[]\n",
      "pleurer\n",
      "[]\n",
      "non merci\n",
      "[]\n",
      "Allo cest bien tel jeunes C'était écrit 24 heures\n",
      "[('Allo', 'PER')]\n",
      "Ok Donc Allo Comment allez vous Allo Allo\n",
      "[('Donc Allo', 'MISC'), ('Allo Allo', 'MISC')]\n",
      "Allo\n",
      "[]\n",
      "Allô tel-jeunes\n",
      "[('Allô', 'LOC')]\n",
      "Jai 18 ans , je suis une fille , Brossard et bon matin : ) \n",
      "[('Brossard', 'PER')]\n",
      "Je me demandais comment faire pour savoir si je suis dépendante à la droge ou l'alcool ? \n",
      "[]\n",
      "Je sais pas , je pense pas mais je commence à consommer de plus en plus Genre depuis 2 semaine j'ai seulement été à jun pendant 2 jours\n",
      "[('Genre', 'MISC')]\n",
      "Ben il est 8h25 du matin , je viens de rentrer chez moi , je travaill e à 11h et ça ne me tente pas de rentrer ... Ça me fait réaliser qu e je préfère mtn plus être Avec mes amis saoul et défoncer que prendre mes responsabilité\n",
      "[('Ben', 'LOC'), ('Ça', 'MISC')]\n",
      "Penser vous que je suis dépendante ? \n",
      "[('Penser', 'PER')]\n",
      "Oui\n",
      "[]\n",
      "Okok\n",
      "[('Okok', 'MISC')]\n",
      "Oui merci\n",
      "[]\n",
      "merci vous aussi Allô tel-jeunes\n",
      "[('Allô', 'LOC')]\n",
      "Non ce n'est pas la première fois et 17 ans fille Longueuil\n",
      "[('Longueuil', 'PER')]\n",
      "Je crois que je commence à avoir un problème de dépendance à la dro gue ... En fait , j'ai peur de ne pas pouvoir m'arrêter quand je vai s recommencer l'école\n",
      "[]\n",
      "Oui ... \n",
      "[]\n",
      "J'ai commencé à fumer beaucoup plus il y a environ 1 mois et demi\n",
      "[]\n",
      "Du h Ben c'est quasi la même chose\n",
      "[]\n",
      "Oui , dans trois jours en fait\n",
      "[]\n",
      "En fait je sais que je consomme par besoin de d'intensité alors j'i magine que je devrais trouver qqch qui me procurer la même chose sa ns être nocif pour moi mais je sais pas quoi ... . \n",
      "[]\n",
      "J'ai besoin de me sentir vivante ces derniers temps , de faire des folies ... \n",
      "[]\n",
      "Comme quoi ? Non pas vraiment ... \n",
      "[]\n",
      "Eum okaie\n",
      "[]\n",
      "C'est pas assez intense\n",
      "[]\n",
      "ue je n'ai pas l'habitude de vivre Je veux me sentir vivante , je veux que ça me procure des émotions q\n",
      "[]\n",
      "Je sais pas , t'es pas dans ton état normal alors c'est drôle et ça fait changement\n",
      "[]\n",
      "Oui , j'ai envie d'essayer d'autres trucs\n",
      "[]\n",
      "C'est vrai . . \n",
      "[]\n",
      "Plus ou moins\n",
      "[]\n",
      "Dacc\n",
      "[]\n",
      "J'ai déjà une psy , elle est en vacances ça fait un mois , je lui en parlerai à son retour\n",
      "[]\n",
      "Ah c'est une bonne idée ça , comme ça au moins il me reste les fin d e semaine\n",
      "[(\"Ah c'est une bonne idée ça ,\", 'MISC')]\n",
      "Ouin j'avoue\n",
      "[]\n",
      "Okaie , merci beaucoup ! : ) \n",
      "[]\n",
      "Allooooo\n",
      "[('Allooooo', 'PER')]\n",
      "Bonjour j'ai une question ? \n",
      "[]\n",
      "15 , féminin sherbooke\n",
      "[]\n",
      "Je voulais savoir , si le Plan B , est vraiment un bon moyen pour \" arrêter \" une grossesese dans les 72h à venir\n",
      "[('Plan B', 'LOC')]\n",
      "Jai eu des relations à risque , dimanche à 3h30 , croyez vous que ça pourrais nuire ? \n",
      "[]\n",
      "D'accord , merci\n",
      "[]\n",
      "Bonjour tel-jeunes , j'ai une question ? \n",
      "[('Bonjour', 'MISC')]\n",
      "Féminin , 15 , saguenay ! \n",
      "[('Féminin', 'LOC')]\n",
      "D'accord , donc hum je sais que la carte étudiante dans les écoles est obligatoire , enfin je le pense . Mais depuis que j'ai environ 11 ans j'ai un problèm point tel que je n'ai pas envie de retourner à l'école . L'an passé , j'avais vomis devant tout le monde et j'avais pleuré car il fallait que je prenne la photo . Pensez-vous qu'il y a un moyen que j'évite de la prendre ? Il n'y a rien au monde que j'aimerais mieux que cela et ce n'est pas que je n'ai pas env ie de la prendre , je ne peux pas . La phobie a été diagnostiquée par mon médecin , etc . J'ai passé 4 ans en thérapie ainsi que 2 ans en hypnose mais rien n e marche . Je ne peux pas le faire , que me conseillez-vous de faire ? e avec les appareils photo , j'en ai une phobie . Cela fait 4 ans que je n'ai pas été prise en photo et prendre une photo de carte étudiante m'intimide au\n",
      "[('Pensez', 'PER')]\n",
      "Parfait , merci . Je vais voir ce que je peux faire de mon côté et je demanderai à mon médecin : ) \n",
      "[('Parfait', 'PER')]\n",
      "Allo Brossard fille 15 ans Un de mes amis me demande . Mais je ne sais pas comment répondre C'est qu'elle a fait une fellation à son chum puis son chum n'avait pas condom sur le penis . Elle demande si elle pouvait être enceinte après avoir fait une fellation\n",
      "[('Allo Brossard', 'PER'), ('Un de mes amis me demande', 'MISC')]\n",
      "D'accord merci\n",
      "[]\n",
      "Et demande combien pour cent qu'elle soit enceinte car elle a pris une douche avec son chum\n",
      "[]\n",
      "D'accord merci D'accord merci\n",
      "[(\"D'accord merci\", 'PER')]\n",
      "C'est tu considérer comme un viol si son chum l'a forcer de faire l'amour ? \n",
      "[]\n",
      "D'accord\n",
      "[]\n",
      "Test vudu Test vudu\n",
      "[('Test vudu', 'ORG')]\n",
      "Oui ! \n",
      "[]\n",
      "Serait-il possible d'envoyer une capture d'écran à jean-rene . auger@vudumobile . ca ? Inclus vos coordonnées au cas ou j'ai besoin de vous parler pour démys tifier le tout . \n",
      "[('auger@vudumobile', 'ORG'), ('Inclus vos', 'PER')]\n",
      "Merci ! Test vudu Test vudu\n",
      "[('Merci !', 'ORG'), ('Test vudu', 'ORG')]\n",
      "Non , merci ! \n",
      "[]\n",
      "Ok , j'y vois a l'instant\n",
      "[]\n",
      "Est-ce qu'il existe un genre de Tel-jeunes pour jeune adulte de 25 ans ? \n",
      "[('Tel', 'LOC')]\n"
     ]
    }
   ],
   "source": [
    "for question in questions[:100]:\n",
    "    doc = tokenize(question)\n",
    "#     print([(token.text, token.lemma_, token.pos_, token.tag_) for token in doc])\n",
    "    print(doc)\n",
    "    print([(ent.text, ent.label_) for ent in doc.ents])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "words_questions = list(itertools.chain.from_iterable(tokenized_questions))\n",
    "words_answers = list(itertools.chain.from_iterable(tokenized_answers))\n",
    "\n",
    "vocab = [('<RAR>',999999), ('<SOS>',999999), ('<EOS>',999999), ('<PAD>',999999), ('<NAME>',999999)] + Counter(words_questions+words_answers).most_common()\n",
    "del words_questions, words_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anonymization and vocab trimming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_Names = []\n",
    "\n",
    "for question in tokenized_questions:\n",
    "    if len(question) > 1:\n",
    "        for word in question[1:]:\n",
    "            if word[0].isupper():\n",
    "                possible_Names.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "min_count = 5\n",
    "counts = [line[1] for line in vocab]\n",
    "\n",
    "possible_Names_sorted = [word for word, count in Counter(possible_Names).most_common()]\n",
    "possible_names_counts = sorted(Counter(possible_Names).values())[::-1]\n",
    "\n",
    "possible_Names_ = [possible_Names_sorted[i] for i, count in enumerate(possible_names_counts) if count >= min_count]\n",
    "\n",
    "# UNCOMMENT FOR SAVING\n",
    "\n",
    "# with open(\"possible_names.csv\", \"w\", encoding='utf-8', newline='') as f:\n",
    "#     csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_NONNUMERIC)\n",
    "#     for name in possible_Names_:\n",
    "#         csv_writer.writerow([name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398\n"
     ]
    }
   ],
   "source": [
    "names = []\n",
    "\n",
    "with open(\"./data/possible_names.csv\", \"r\", encoding='utf-8', newline='') as f:\n",
    "    csv_reader = csv.reader(f, delimiter=',')\n",
    "    for line in csv_reader:\n",
    "        if len(line) > 1:\n",
    "            if line[1] == '1':\n",
    "                names.append(line[0].lower())     \n",
    "print(len(names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update vocab with lowercases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 114792/114792 [02:52<00:00, 666.50it/s]\n"
     ]
    }
   ],
   "source": [
    "tmp = vocab[:5]\n",
    "for word, count in vocab[5:]:\n",
    "    tmp.append((word.lower(),count))\n",
    "print(len(tmp))\n",
    "    \n",
    "vocab_lower_doublon = [word for word,count in Counter([word for word, count in tmp]).most_common() if count>1]\n",
    "\n",
    "vocab = []\n",
    "for j, (word, count) in enumerate(tqdm(tmp)):\n",
    "    if word in vocab_lower_doublon:\n",
    "        try:\n",
    "            vocab_lower_words = [word for word, count in vocab]\n",
    "            i = vocab_lower_words.index(word)\n",
    "            vocab[i] = (word,count+vocab[i][1])\n",
    "        except:\n",
    "            vocab.append((word, count))\n",
    "    else:\n",
    "        vocab.append((word, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/vocab.csv', 'w', newline='', encoding=encoding) as csvfile:\n",
    "    vocab_writer = csv.writer(csvfile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_NONNUMERIC)\n",
    "    for (word, count) in vocab:\n",
    "        vocab_writer.writerow([word,count])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Q_A with vocab indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_indexer = dict()\n",
    "for i, (word, count) in enumerate(vocab):\n",
    "    if count >= min_count:\n",
    "        if not word in names:\n",
    "            vocab_indexer[word] = i\n",
    "        else:\n",
    "            vocab_indexer[word] = 4\n",
    "    else:\n",
    "        vocab_indexer[word] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_A_indexed = []\n",
    "\n",
    "for i, (context, question, answer, subject) in enumerate(Q_A):\n",
    "    question_indexed = [vocab_indexer[word.lower()] for word in question]\n",
    "    answer_indexed = [vocab_indexer[word.lower()] for word in answer]\n",
    "    standard_subject = subject[:]\n",
    "    if len(subject) < 2:\n",
    "        standard_subject = ['', '']\n",
    "    Q_A_indexed.append([context, question_indexed, answer_indexed, standard_subject])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION: <RAR>\n",
      "ANSWER: bonjour comment puis-je t'aider <RAR>\n",
      "SUBJECT:  ['', '']\n",
      "QUESTION: comment vous savez mon nom ? ? ?\n",
      "ANSWER: c'est écrit dans mon écran : )\n",
      "SUBJECT:  ['', '']\n",
      "QUESTION: ah bon , on a plus de secret maintenant\n",
      "ANSWER: ça a l'air que non ! bonne chance\n",
      "SUBJECT:  ['', '']\n",
      "QUESTION: allo ok merci d'être là : )\n",
      "ANSWER: allo bon matin , le texto est maintenant ouvert . nous sommes la jusqu , a 23h si tu veux discuter avec nous : )\n",
      "SUBJECT:  ['Ça va pas', '']\n",
      "QUESTION: je me sens toute seule\n",
      "ANSWER: pour commencer , accepterais tu de me donner ton age et ta ville ? c pour nos stats , mais ca m'aide tjrs a me situer dans la conversation : )\n",
      "SUBJECT:  ['Ça va pas', '']\n",
      "QUESTION: <NAME> , <NAME> , <NAME>\n",
      "ANSWER: merci : ) pourrais tu m'en dire un peu plus sur ta situation ? pourquoi te sens-tu seule ?\n",
      "SUBJECT:  ['Ça va pas', '']\n",
      "QUESTION: car j'avais une amie qui se nomme <RAR> puis nous ne sommes plus amies car elle se fâchait régulièrement envers moi . elle dit qu'elle a une tête de cochon . puis quand je lui met en pleine face que c'est ça la réalité elle ne comprend pas . puis elle fait des crises de colère . mais moi je dis seulement que je suis gentille avec elle .\n",
      "ANSWER: c jamais évident les chicanes d'amis . dis-moi , comment aimerais tu que je t'aide dans cette situation ?\n",
      "SUBJECT:  ['Ça va pas', '']\n",
      "QUESTION: j'aimerais que vous m'aidiez en me disant qu'elle serait la bonne chose à faire ?\n",
      "ANSWER: pour une chicane , ca dépend de ce qu , on veut faire , ex essayer de redevenir amis ou passer a autre chose . et toi , tu veux quoi ?\n",
      "SUBJECT:  ['Ça va pas', '']\n",
      "QUESTION: passer à autre chose car ce n'est pas la première fois que elle me fait le coup .\n",
      "ANSWER: je comprends . tu sais , passer a autre chose , ca peut se faire en prenant une distance avec la pers . et en meme temps , ca peut nous faire vivre des émotions , comme tristesse , colere , etc . on peut parfois vivre une peine d'amitié . et ca prend un temps\n",
      "SUBJECT:  ['Ça va pas', '']\n",
      "QUESTION: oui mais on habite l'une près de l'autre\n",
      "ANSWER: moins évident dans ce contexte ... ca demande de s'habituer a se croiser , sans trop se parler ou autre ... si c ce qu'on veut bien sur : ) on peut aussi simplement saluer la pers , sans plus .\n",
      "SUBJECT:  ['Ça va pas', '']\n"
     ]
    }
   ],
   "source": [
    "for context, question, answer, subject in Q_A_indexed[:10]:\n",
    "    print('QUESTION:', ' '.join([vocab[index][0] for index in question]))\n",
    "    print('ANSWER:', ' '.join([vocab[index][0] for index in answer]))\n",
    "    print('SUBJECT: ', subject)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/Q_A.npy'\n",
    "with open(path, 'wb') as file:\n",
    "    pickle.dump(Q_A_indexed, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading snippet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Conversation csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pickle\n",
    "\n",
    "vocab_indexer = dict()\n",
    "vocab = []\n",
    "counts = []\n",
    "\n",
    "with open('./data/vocab.csv', 'r', newline='', encoding='utf-8') as csvfile:\n",
    "    vocab_reader = csv.reader(csvfile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_NONNUMERIC)\n",
    "    i = 0\n",
    "    for word, count in vocab_reader:\n",
    "        vocab_indexer[word] = i\n",
    "        vocab.append(word)\n",
    "        counts.append(count)\n",
    "        i += 1\n",
    "\n",
    "with open('./data/Q_A.npy', 'rb') as file:\n",
    "    Q_A = pickle.load(file)\n",
    "\n",
    "questions = []\n",
    "answers = []\n",
    "contextes = []\n",
    "subjects = []\n",
    "\n",
    "for i, line in enumerate(Q_A):\n",
    "    if len(line[2]) > 0:\n",
    "        questions.append([vocab[index] for index in line[1][:]])\n",
    "        answers.append([vocab[index] for index in line[2][:]])\n",
    "        contextes.append(line[0][:])\n",
    "        subjects.append(line[3][:])\n",
    "    else:\n",
    "        print('ERROR')\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "import pickle\n",
    "\n",
    "vocab_indexer = dict()\n",
    "vocab = []\n",
    "counts = []\n",
    "\n",
    "with open('./data/vocab.csv', 'r', newline='', encoding='utf-8') as csvfile:\n",
    "    vocab_reader = csv.reader(csvfile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_NONNUMERIC)\n",
    "    i = 0\n",
    "    for word, count in vocab_reader:\n",
    "        vocab_indexer[word] = i\n",
    "        vocab.append(word)\n",
    "        counts.append(count)\n",
    "        i += 1\n",
    "\n",
    "with open('./data/Q_A.npy', 'rb') as file:\n",
    "    Q_A = pickle.load(file)\n",
    "\n",
    "convos = []\n",
    "conv_no = 0\n",
    "convo = []\n",
    "\n",
    "for i, line in enumerate(Q_A[:100]):\n",
    "    if len(line[2]) > 0:\n",
    "        if len(line[0]) == 0:\n",
    "            conv_no += 1;\n",
    "            convos.append(convo)\n",
    "            convo = []\n",
    "        else:\n",
    "            question = ' '.join([vocab[index] for index in line[1][:]])\n",
    "            answer = ' '.join([vocab[index] for index in line[2][:]])\n",
    "            convo.append({\"question\":question, \"answer\":answer})\n",
    "    else:\n",
    "        print('ERROR')\n",
    "        raise\n",
    "convos.append(convo)\n",
    "print(conv_no)\n",
    "with open(\"./convos.json\", 'w', encoding='utf-8') as f: \n",
    "    json.dump(convos, f, sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_no = -1\n",
    "\n",
    "with open('./data/conversations.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    conv_writer = csv.writer(csvfile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_NONNUMERIC)\n",
    "    for i, line in enumerate(questions):\n",
    "        if len(contextes[i]) == 0:\n",
    "            conv_no+=1\n",
    "        question = ' '.join([word for word in line])\n",
    "        answer = ' '.join([word for word in answers[i]])\n",
    "        conv_writer.writerow([conv_no, question, answer, subjects[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create file for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pickle\n",
    "\n",
    "vocab_indexer = dict()\n",
    "vocab = []\n",
    "counts = []\n",
    "\n",
    "with open('./data/vocab.csv', 'r', newline='', encoding='utf-8') as csvfile:\n",
    "    vocab_reader = csv.reader(csvfile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_NONNUMERIC)\n",
    "    i = 0\n",
    "    for word, count in vocab_reader:\n",
    "        vocab_indexer[word] = i\n",
    "        vocab.append(word)\n",
    "        counts.append(count)\n",
    "        i += 1\n",
    "\n",
    "with open('./data/Q_A.npy', 'rb') as file:\n",
    "    Q_A = pickle.load(file)\n",
    "\n",
    "questions = []\n",
    "contextes = []\n",
    "subjects = []\n",
    "\n",
    "for i, line in enumerate(Q_A):\n",
    "    if len(line[2]) > 0:\n",
    "        contextes.append(line[0][:])\n",
    "        questions.append([vocab[index] for index in line[1][:]])\n",
    "        subjects.append(line[3][:])\n",
    "    else:\n",
    "        print('ERROR')\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'Ça va pas', 'Amour', 'Drogues et alcool', 'Sexualité', 'Amis et famille', 'Technos', 'Academic life', 'Bien-être', 'Social health', ' ']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "subject_to_int = list(Counter([line[0] for line in subjects]))\n",
    "print(subject_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_no = -1\n",
    "whole_question = ''\n",
    "subject = ['', '']\n",
    "\n",
    "with open('./data/data_fr.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    conv_writer = csv.writer(csvfile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_NONNUMERIC)\n",
    "    for i, line in enumerate(questions):\n",
    "        question = ' '.join([word for word in line])\n",
    "        if len(contextes[i]) == 0:\n",
    "            conv_no+=1\n",
    "            if whole_question == '':\n",
    "                whole_question = question\n",
    "            data = [whole_question]\n",
    "            data.extend(subject)\n",
    "            conv_writer.writerow(data)\n",
    "            whole_question = question\n",
    "            subject = subjects[i]\n",
    "        elif len(contextes[i]) < 5:\n",
    "            whole_question += (' '+question)\n",
    "        else:\n",
    "            conv_writer.writerow([question, '', ''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 265917/265917 [01:46<00:00, 2488.72it/s]\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import FrenchStemmer\n",
    "from tqdm import tqdm\n",
    "\n",
    "stemmer = FrenchStemmer(ignore_stopwords=False)\n",
    "\n",
    "stemmed_questions = []\n",
    "\n",
    "for question in tqdm(questions):\n",
    "    words = [stemmer.stem(token) for token in question]\n",
    "    stemmed_questions.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_no = -1\n",
    "whole_question = ''\n",
    "subject = ['', '']\n",
    "\n",
    "with open('./data/stemmed_data_fr.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    conv_writer = csv.writer(csvfile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_NONNUMERIC)\n",
    "    for i, line in enumerate(stemmed_questions):\n",
    "        question = ' '.join([word for word in line])\n",
    "        if len(contextes[i]) == 0:\n",
    "            conv_no+=1\n",
    "            if whole_question == '':\n",
    "                whole_question = question\n",
    "            data = [whole_question]\n",
    "            data.extend(subject)\n",
    "            conv_writer.writerow(data)\n",
    "            whole_question = question\n",
    "            subject = subjects[i]\n",
    "        elif len(contextes[i]) < 5:\n",
    "            whole_question += (' '+question)\n",
    "        else:\n",
    "            conv_writer.writerow([question, '', ''])\n",
    "            \n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "words_questions = list(itertools.chain.from_iterable(stemmed_questions))\n",
    "\n",
    "vocab = [('<RAR>',999999), ('<SOS>',999999), ('<EOS>',999999), ('<PAD>',999999), ('<NAME>',999999)] + Counter(words_questions).most_common()\n",
    "del words_questions\n",
    "\n",
    "with open('./data/stemmed_vocab.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    vocab_writer = csv.writer(csvfile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_NONNUMERIC)\n",
    "    for (word, count) in vocab:\n",
    "        vocab_writer.writerow([word,count])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/Q_A.csv'\n",
    "bar = progressbar.ProgressBar(max_value=len(Q_A))\n",
    "\n",
    "with open(path, 'w', newline='', encoding=encoding) as csvfile:\n",
    "    conv_writer = csv.writer(csvfile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_NONNUMERIC)\n",
    "    for i, line in enumerate(Q_A):\n",
    "        if i % int(len(Q_A)/100) == 0:\n",
    "            bar.update(i)\n",
    "        conv_writer.writerow(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = []\n",
    "import csv\n",
    "\n",
    "with open(\"./data/subject_sms.txt\", encoding='ANSI') as f:\n",
    "    for line in f:\n",
    "        elems = line.split(\"\\t\")\n",
    "        subject = [elems[0], elems[1], elems[2][:-1]]\n",
    "        subjects.append(subject)      \n",
    "with open('./data/subject_sms.csv', 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
    "    csv_writer = csv.writer(csvfile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_NONNUMERIC)\n",
    "    for line in subjects:\n",
    "        csv_writer.writerow(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
